<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.111.0">
<title>Understanding the Importance of Model Explainability in Machine Learning | Outlier Blog</title>








  
    
  
<meta name="description" content="Data Science | Artificial Intelligence | Programming">


<meta property="og:site_name" content="Outlier Blog">
<meta property="og:title" content="Understanding the Importance of Model Explainability in Machine Learning | Outlier Blog">
<meta property="og:description" content="Data Science | Artificial Intelligence | Programming" />
<meta property="og:type" content="page" />
<meta property="og:url" content="/blog/2023-03-08-understanding-the-importance-of-model-explainability-in-machine-learning/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="/blog/2023-03-08-understanding-the-importance-of-model-explainability-in-machine-learning/featured.jpg" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="/blog/2023-03-08-understanding-the-importance-of-model-explainability-in-machine-learning/featured.jpg" >
    
    
  <meta itemprop="name" content="Understanding the Importance of Model Explainability in Machine Learning">
<meta itemprop="description" content="Machine learning has become an essential tool for businesses and organizations to gain insights and make predictions based on large amounts of data. It has been widely adopted across industries to solve problems, optimize processes, and make more accurate predictions. However, as machine learning models become more complex, it becomes increasingly difficult to understand how they arrive at their decisions. Put simply, model explainability refers to the ability to understand and explain how a machine learning model arrives at its predictions or decisions."><meta itemprop="datePublished" content="2023-03-08T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-03-08T00:00:00+00:00" />
<meta itemprop="wordCount" content="824"><meta itemprop="image" content="/blog/2023-03-08-understanding-the-importance-of-model-explainability-in-machine-learning/featured.jpg">
<meta itemprop="keywords" content="" />
  
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/img/favicon.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.51ef9a26ed5e31959c31ea456a050c05fbf3938c97c284a4438715138fcb2e8d.css" integrity="sha256-Ue&#43;aJu1eMZWcMepFagUMBfvzk4yXwoSkQ4cVE4/LLo0=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.649b37902a58940ccb0994180039a0295d71785e8ede8221f0c9fb5bf73c9354.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container single">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="/" title="Home">
      <img src="/img/logo.png" class="dib db-l h2 w-auto" alt="Outlier Blog">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About Me">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Understanding the Importance of Model Explainability in Machine Learning</h1>
        
        <p class="f6 measure lh-copy mv1">By Thomas Shaw in <a href="/categories/machine-learning">machine learning</a> </p>
        <p class="f7 db mv0 ttu">March 8, 2023</p>

      

      </header>
      <section class="post-body pt5 pb4">
        <p>Machine learning has become an essential tool for businesses and organizations to gain insights and make predictions based on large amounts of data. It has been widely adopted across industries to solve problems, optimize processes, and make more accurate predictions. However, as machine learning models become more complex, it becomes increasingly difficult to understand how they arrive at their decisions. Put simply, model explainability refers to the ability to understand and explain how a machine learning model arrives at its predictions or decisions. It&rsquo;s important for a variety of reasons, from building trust in the model&rsquo;s predictions to ensuring compliance with legal requirements. In this blog post, we&rsquo;ll explore the concept of model explainability in more detail, and discuss some common methods for achieving it.</p>




<h2 id="why-is-model-explainability-important">Why is Model Explainability Important
  <a href="#why-is-model-explainability-important"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>




<h3 id="1-transparency-and-trust">1. Transparency and Trust
  <a href="#1-transparency-and-trust"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>One of the main reasons why model explainability is so important is because it allows us to be transparent. This is particularly crucial in areas such as healthcare, finance, and justice, where people are directly impacted by the decisions made by machine learning models. When a model makes a decision that affects someone&rsquo;s health, financial wellbeing, or legal rights, they want to know how that decision was made and what factors were taken into account. Model explainability also helps build trust in the model&rsquo;s predictions. If stakeholders understand how a model works and how it came to its conclusions, they are more likely to trust it. This is especially important when making high-stakes decisions that could have significant impacts on people&rsquo;s lives.</p>




<h3 id="2-compliance-and-regulation">2. Compliance and Regulation
  <a href="#2-compliance-and-regulation"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Another reason why model explainability is important is compliance and regulation. Model explainability is becoming a legal requirement in some jurisdictions, such as the European Union&rsquo;s General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). These regulations often require that models be transparent and explainable, so that users can understand how the model arrived at its decision. Failure to comply with these regulations can result in legal and financial consequences.</p>




<h3 id="3-debugging-and-improvement">3. Debugging and Improvement
  <a href="#3-debugging-and-improvement"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The third reason why model explainability is important is debugging and improvement. When a model is not performing as expected, it can be difficult to identify the root cause of the problem. Understanding how a model works can help data scientists identify errors and improve the model&rsquo;s performance.</p>




<h2 id="how-to-achieve-model-explainability">How to Achieve Model Explainability
  <a href="#how-to-achieve-model-explainability"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>




<h3 id="1-feature-importance">1. Feature Importance
  <a href="#1-feature-importance"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Feature importance is a method that tells us which features in our data are most important for our model&rsquo;s predictions. We can use this information to understand which factors are driving the model&rsquo;s decisions. We can use the **<code>feature_importances_</code>**attribute of scikit-learn&rsquo;s RandomForestRegressor or RandomForestClassifier to extract feature importance scores.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.ensemble</span> <span style="color:#000;font-weight:bold">import</span> RandomForestClassifier
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.datasets</span> <span style="color:#000;font-weight:bold">import</span> make_classification
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Generate synthetic dataset</span>
</span></span><span style="display:flex;"><span>X, y <span style="color:#000;font-weight:bold">=</span> make_classification(n_samples<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1000</span>, n_features<span style="color:#000;font-weight:bold">=</span><span style="color:#099">10</span>, n_informative<span style="color:#000;font-weight:bold">=</span><span style="color:#099">5</span>, n_redundant<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>, random_state<span style="color:#000;font-weight:bold">=</span><span style="color:#099">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Train a Random Forest model</span>
</span></span><span style="display:flex;"><span>rf <span style="color:#000;font-weight:bold">=</span> RandomForestClassifier(n_estimators<span style="color:#000;font-weight:bold">=</span><span style="color:#099">100</span>, random_state<span style="color:#000;font-weight:bold">=</span><span style="color:#099">42</span>)
</span></span><span style="display:flex;"><span>rf<span style="color:#000;font-weight:bold">.</span>fit(X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Get feature importances</span>
</span></span><span style="display:flex;"><span>importances <span style="color:#000;font-weight:bold">=</span> rf<span style="color:#000;font-weight:bold">.</span>feature_importances_
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Print feature importances</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">for</span> feature, importance <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">zip</span>(<span style="color:#0086b3">range</span>(X<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">1</span>]), importances):
</span></span><span style="display:flex;"><span>    <span style="color:#0086b3">print</span>(<span style="color:#d14">f</span><span style="color:#d14">&#34;Feature </span><span style="color:#d14">{</span>feature<span style="color:#d14">}</span><span style="color:#d14">: </span><span style="color:#d14">{</span>importance<span style="color:#d14">}</span><span style="color:#d14">&#34;</span>)
</span></span></code></pre></div>



<h3 id="2-partial-dependence-plots">2. Partial Dependence Plots
  <a href="#2-partial-dependence-plots"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Partial dependence plots show the relationship between a feature and the model&rsquo;s predictions while holding all other features constant. They can be used to understand how a feature affects the model&rsquo;s output, and to identify any non-linear relationships between the features and the target variable. We can use scikit-learn&rsquo;s **<code>plot_partial_dependence</code>**function to create partial dependence plots.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.ensemble</span> <span style="color:#000;font-weight:bold">import</span> RandomForestClassifier
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.datasets</span> <span style="color:#000;font-weight:bold">import</span> load_breast_cancer
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.inspection</span> <span style="color:#000;font-weight:bold">import</span> plot_partial_dependence
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Load the Breast Cancer dataset</span>
</span></span><span style="display:flex;"><span>data <span style="color:#000;font-weight:bold">=</span> load_breast_cancer()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Train a Random Forest model</span>
</span></span><span style="display:flex;"><span>rf <span style="color:#000;font-weight:bold">=</span> RandomForestClassifier(n_estimators<span style="color:#000;font-weight:bold">=</span><span style="color:#099">100</span>, random_state<span style="color:#000;font-weight:bold">=</span><span style="color:#099">42</span>)
</span></span><span style="display:flex;"><span>rf<span style="color:#000;font-weight:bold">.</span>fit(data<span style="color:#000;font-weight:bold">.</span>data, data<span style="color:#000;font-weight:bold">.</span>target)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Create partial dependence plots for two features</span>
</span></span><span style="display:flex;"><span>features <span style="color:#000;font-weight:bold">=</span> [<span style="color:#099">0</span>, <span style="color:#099">7</span>] <span style="color:#998;font-style:italic"># radius mean and concavity mean</span>
</span></span><span style="display:flex;"><span>plot_partial_dependence(rf, data<span style="color:#000;font-weight:bold">.</span>data, features, feature_names<span style="color:#000;font-weight:bold">=</span>data<span style="color:#000;font-weight:bold">.</span>feature_names)
</span></span></code></pre></div>



<h3 id="3-shap-values">3. SHAP Values
  <a href="#3-shap-values"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>SHAP (SHapley Additive exPlanations) values provide an explanation for each feature&rsquo;s contribution to the model&rsquo;s prediction. We can use the **<code>shap</code>**library to compute SHAP values for a model trained with XGBoost.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">xgboost</span> <span style="color:#000;font-weight:bold">as</span> <span style="color:#555">xgb</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">shap</span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">sklearn.datasets</span> <span style="color:#000;font-weight:bold">import</span> load_boston
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Load the Boston Housing dataset</span>
</span></span><span style="display:flex;"><span>data <span style="color:#000;font-weight:bold">=</span> load_boston()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Train an XGBoost model</span>
</span></span><span style="display:flex;"><span>xgb_model <span style="color:#000;font-weight:bold">=</span> xgb<span style="color:#000;font-weight:bold">.</span>train({<span style="color:#d14">&#34;learning_rate&#34;</span>: <span style="color:#099">0.01</span>}, xgb<span style="color:#000;font-weight:bold">.</span>DMatrix(data<span style="color:#000;font-weight:bold">.</span>data, label<span style="color:#000;font-weight:bold">=</span>data<span style="color:#000;font-weight:bold">.</span>target), <span style="color:#099">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Compute SHAP values for a single instance</span>
</span></span><span style="display:flex;"><span>explainer <span style="color:#000;font-weight:bold">=</span> shap<span style="color:#000;font-weight:bold">.</span>Explainer(xgb_model)
</span></span><span style="display:flex;"><span>shap_values <span style="color:#000;font-weight:bold">=</span> explainer(data<span style="color:#000;font-weight:bold">.</span>data[<span style="color:#099">0</span>,:])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># Plot the SHAP values for the instance</span>
</span></span><span style="display:flex;"><span>shap<span style="color:#000;font-weight:bold">.</span>plots<span style="color:#000;font-weight:bold">.</span>waterfall(shap_values[<span style="color:#099">0</span>])
</span></span></code></pre></div>



<h3 id="4-interpretable-models">4. Interpretable Models
  <a href="#4-interpretable-models"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Another approach to achieving model explainability is to use interpretable models, such as decision trees or linear regression. These models are generally more transparent because their decision-making process is easier to understand. While interpretable models may not always perform as well as more complex models, they can be a useful tool for understanding how a model is making its decisions.</p>




<h2 id="conclusion">Conclusion
  <a href="#conclusion"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>In conclusion, model explainability is a crucial aspect of machine learning that should not be ignored. It helps in establishing trust in the model&rsquo;s predictions, promoting transparency and accountability, and may even be a legal requirement in some instances. By prioritizing model explainability, data scientists and machine learning practitioners can develop models that are not only accurate and effective, but also transparent and trustworthy.</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">March 8, 2023</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">4 minute read, 824 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="/categories/machine-learning">machine learning</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="/blog/2023-03-09-4-hyperparameters-optimization-frameworks-for-machine-learning/">&larr; 4 Hyperparameters Optimization Frameworks for Machine Learning</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="/blog/2023-03-02-understanding-confusion-matrix/">Understanding Confusion Matrix &rarr;</a>
  
</div>

      </footer>
    </article>
    
  </section>
  	<div class="relative flex py-5 items-center">
	</div>
	<div class="flex justify-center">
		<p style="margin-bottom: 1em;"><b>Support my work with a cup of ☕</b></p>
	</div>
	<div class="flex justify-center">
		<script type='text/javascript' src='https://storage.ko-fi.com/cdn/widget/Widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Donate', '#4c7021', 'N4N1EM0G5');kofiwidget2.draw();</script>
	</div>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2023 Thomas Shaw
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/thomassshaw" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://medium.com/@thomassshaw" title="medium" target="_blank" rel="me noopener">
      <i class="fab fa-medium fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="/blog/index.xml" title="rss" >
      <i class="fas fa-rss fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
